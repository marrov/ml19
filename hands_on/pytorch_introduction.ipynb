{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch tutorial\n",
    "*Adapted from [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html \"Pytorch tutorial\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is:  \n",
    "* A deep learning framework (mainly geared towards research)  \n",
    "* A GPU-powered numpy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from Numpy ndarrays, Tensors can be used on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,4, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7490, 0.5845, 0.5823, 0.2272],\n",
      "        [0.8460, 0.0612, 0.4490, 0.4116]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2000, 4.0000],\n",
      "        [3.0000, 5.6000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.2, 4],[3, 5.6]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be easily manipulated using usual operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[1.8530, 4.0379],\n",
      "        [3.2658, 5.7057]])\n",
      "tensor([[1.8530, 4.0379],\n",
      "        [3.2658, 5.7057]])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "\n",
    "y = torch.rand(2,2)\n",
    "print(x + y)\n",
    "# alternatively\n",
    "z =  torch.empty(2,2)\n",
    "torch.add(x,y, out=z)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8530, 4.0379],\n",
      "        [3.2658, 5.7057]])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(z)\n",
    "print(z.size())\n",
    "batch_size = 1\n",
    "# to reshape a multi-dimensional array\n",
    "z = z.view(-1,batch_size)     # the size -1 can be used, the dimension is inferred from the others\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Numpy to Pytorch variables share the memory location, hence we can easily **modify** them, as long as they are on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.],\n",
      "        [4., 4.]])\n",
      "[[4. 4.]\n",
      " [4. 4.]\n",
      " [4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3,2)\n",
    "print(a)\n",
    "\n",
    "# from Pytorch to Numpy\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "a.add_(3)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# from Numpy to Pytorch\n",
    "c = torch.from_numpy(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``autograd`` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,4, dtype=torch.float)\n",
    "print(x)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True)\n",
    "print(x)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8., 8., 8., 8.],\n",
      "        [8., 8., 8., 8.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 7\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Tensor``.\n",
    "Other important methods:  \n",
    "* ``.backward()``: gradients are computed automatically\n",
    "* ``.detach()``: future computations are not tracked\n",
    "\n",
    "We can also prevent tracking history wrapping the code in a block in ``with torch.no_grad():``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of the ``torch.nn`` package. An ``nn.Module`` contains layers, and a method ``forward(input)`` that returns the ``output``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist for neural networks\n",
    "1. Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "2. Iterate over a dataset of inputs, for each input:\n",
    "  1. Process input through the network\n",
    "  2. Compute the loss (how far is the output from being correct)\n",
    "  3. Propagate gradients back into the networkâ€™s parameters\n",
    "  4. Update the weights of the network, using some stochastic gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Example of network definition and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_MNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net_MNIST(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1):\n",
    "        super(Net_MNIST, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "# create a Net object, i.e. our neural network\n",
    "input_dim = 28**2   # assuming as input an 28x28 image\n",
    "hidden_dim = 30\n",
    "batch_size = 1\n",
    "output_dim = 10\n",
    "\n",
    "net = Net_MNIST(input_dim=input_dim, hidden_dim=hidden_dim, batch_size=batch_size, output_dim=output_dim)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each mini-batch of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Processing input\n",
    "The network is called as a function, using the input batch data to get the output batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.2330, 0.0000, 0.0434, 0.0000, 0.0000, 0.0222, 0.0495,\n",
      "         0.2554]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(batch_size, 28, 28)\n",
    "out = net(inp.view(-1,input_dim)) # Reshaping as the NN wants it\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Computing loss\n",
    "There are a number of pre-defined loss functions, but it is possible to define our own.\n",
    "It is important that, given the prediction $y_{pred} \\in \\mathbb{R}^d$ and the ground truth $y_{true} \\in \\mathbb{R}^d$, the loss $\\mathcal{L}$ is defined so that\n",
    "\n",
    "$$ \\mathcal{L} : \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0010, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.randn(10)         # a dummy target, for example\n",
    "target = target.view(1, -1)      # make it the same shape as output\n",
    "criterion = nn.MSELoss()         # MSE = mean squared error - the usuall/typical one\n",
    "\n",
    "loss = criterion(out, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Backpropagation\n",
    "The computation of the gradient is done automatically using the ``.backward()`` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Update the weights\n",
    "We need to choose an optimization algorithm, different options are available as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in the training loop:\n",
    "optimizer.zero_grad()             # zero the gradient buffers\n",
    "out = net(inp.view(-1,input_dim))\n",
    "loss = criterion(out, target)\n",
    "loss.backward()\n",
    "optimizer.step()                  # does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
