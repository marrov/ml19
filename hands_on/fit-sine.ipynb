{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session I: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#plt.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are provided as equally spaced points in the interval $[0,1]$, from the function $\\sin{2\\pi x}$, adding a small level of random noise with Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 10                                                         # Number of datapoints\n",
    "x_draw = np.linspace(0,1,100)                                  # x-points to draw the sine function\n",
    "x = np.linspace(0,1,N)                                         # Equally-spaced points in the interval\n",
    "t = np.sin(2*math.pi*x) + 0.3*np.random.normal(size=(N,))      # Datapoints\n",
    "t_test = np.sin(2*math.pi*x) + 0.3*np.random.normal(size=(N,)) # Datapoints for testing\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x,t,'o')\n",
    "ax.plot(x_draw,np.sin(2*math.pi*x_draw),'g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to fit the data using a polynomial function of order $M$, minimizing the mean-squared error with respect to the provided data\n",
    "\n",
    "$$ y(x,\\mathbf{w}) = w_0 + w_1x + \\dotsc + w_Mx^M = \\sum^M_{j=0} w_jx^j $$\n",
    "\n",
    "$$ E(\\mathbf{w}) = \\frac{1}{2} \\sum^N_{n=1} \\{ y(x_n, \\mathbf{w}) - t_n \\}^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_coeff = lambda x,M: np.array([x**i for i in range(M+1)])\n",
    "poly_eval = lambda x,w: np.sum(w*poly_coeff(x, len(w)-1))\n",
    "\n",
    "def OLS(M):\n",
    "    X = np.zeros((N,M+1), dtype='float')\n",
    "    for i_sample in range(N):\n",
    "        X[i_sample] = poly_coeff(x[i_sample],M)\n",
    "    \n",
    "    w = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T,X)),X.T),t)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4                                                        # Degree of the polynomial\n",
    "\n",
    "w_OLS = OLS(M)\n",
    "y_OLS = np.array([poly_eval(x_draw[i], w_OLS) for i in range(len(x_draw))])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x,t,'o')\n",
    "ax.plot(x_draw,np.sin(2*math.pi*x_draw),'g')\n",
    "ax.plot(x_draw,y_OLS,'r')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print(w_OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMS = lambda w,x,t: np.sum(np.array([poly_eval(x[i], w)-t[i] for i in range(len(x))])**2)/len(x)\n",
    "\n",
    "W = list()\n",
    "Y_OLS = np.ndarray((N,len(x_draw)), dtype='float')\n",
    "E_RMS_training = np.ndarray((N,), dtype='float')\n",
    "E_RMS_test = np.ndarray((N,), dtype='float')\n",
    "for i_M in range(N):\n",
    "    w_OLS = OLS(i_M)\n",
    "    W.append(w_OLS)\n",
    "    Y_OLS[i_M] = np.array([poly_eval(x_draw[i], w_OLS) for i in range(len(x_draw))])\n",
    "    E_RMS_training[i_M] = RMS(w_OLS,x,t)\n",
    "    E_RMS_test[i_M] = RMS(w_OLS,x,t_test)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x,t,'o')\n",
    "ax.plot(x,t_test,'^')\n",
    "ax.plot(x_draw,np.sin(2*math.pi*x_draw),'g')\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(E_RMS_training,'-o')\n",
    "ax.plot(E_RMS_test,'-o')        \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (0,1,3,9):    \n",
    "    print(W[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L_2$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the magnitude of the model parameters we can add *regularization*\n",
    "\n",
    "$$ E(\\mathbf{w}) = \\frac{1}{2} \\sum^N_{n=1} \\{ y(x_n, \\mathbf{w}) - t_n \\}^2 + \\frac{\\lambda}{2} \\| \\mathbf{w} \\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_wL2(M, loglamb):\n",
    "    X = np.zeros((N,M+1), dtype='float')\n",
    "    for i_sample in range(N):\n",
    "        X[i_sample] = poly_coeff(x[i_sample],M)\n",
    "    L2_matrix = np.exp(loglamb)*np.eye(M+1)\n",
    "    L2_matrix[0,0] = 0\n",
    "    w = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T,X)+L2_matrix),X.T),t)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1                                                        # Degree of the polynomial\n",
    "loglamb = -10                                                # Log of the regularization parameter  \n",
    "\n",
    "w_L2 = OLS_wL2(M, loglamb)\n",
    "y_L2 = np.array([poly_eval(x_draw[i], w_L2) for i in range(len(x_draw))])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x,t,'o')\n",
    "ax.plot(x_draw,np.sin(2*math.pi*x_draw),'g')\n",
    "ax.plot(x_draw,y_L2,'r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_L2) # M = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_L2) # M = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things that I am skipping\n",
    "* More complex linear models\n",
    "* Multivariate models\n",
    "* Bayesian approach\n",
    "* *etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1\n",
    "\n",
    "w1 = np.linspace(-3,3,100)\n",
    "w2 = np.linspace(-3,3,100)\n",
    "err_map = np.zeros((len(w1),len(w2)), dtype='float')\n",
    "for i_1 in range(len(w1)):\n",
    "    for i_2 in range(len(w2)):\n",
    "        w_map = np.array([w1[i_1], w2[i_2]])\n",
    "        err_map[i_1,i_2] = RMS(w_map,x,t)\n",
    "        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)        \n",
    "ax.contour(w1,w2,err_map,40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the error function $E(\\mathbf{w})$ and an initial guess $\\mathbf{w}^{(0)}$, the parameters of the model are updated iteratively\n",
    "\n",
    "$$ \\mathbf{w}^{(i+1)} = \\mathbf{w}^{(i)} - \\eta\\frac{\\partial E(\\mathbf{w}^{(i)})}{\\partial \\mathbf{w}^{(i)}} \\quad \\text{for }i=1,2,\\dotsc $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)        \n",
    "ax.contour(w1,w2,err_map,40)\n",
    "\n",
    "w_GD = np.random.normal(size=(M+1,))\n",
    "ax.plot(w_GD[1],w_GD[0],'ro')\n",
    "ax.plot(w_L2[1],w_L2[0],'r*')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For stochastic gradient descent, we consider the error function without regularization and we rewrite it as\n",
    "\n",
    "$$ E(\\mathbf{w}) = \\sum^N_{n=1} E_n(\\mathbf{w}) = \\sum^N_{n=1} \\frac{1}{2} \\{ y(x_n, \\mathbf{w}) - t_n \\}^2 $$\n",
    "\n",
    "then we approximate it with\n",
    "\n",
    "$$ E(\\mathbf{w}) \\approx \\sum^{N_b}_{n=1} E_n(\\mathbf{w}) $$\n",
    "\n",
    "where $N_b \\ll N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run again the regularized optimization with M = 5\n",
    "M = 5\n",
    "\n",
    "lr = 0.05\n",
    "tol = 1e-5\n",
    "\n",
    "w_SGD = np.random.normal(size=(M+1,)) # Initialization of the weight array\n",
    "delta_W = np.ones((M+1,),dtype='float')\n",
    "samples = range(N)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x,t,'o')\n",
    "n_iter = 0\n",
    "err = list()\n",
    "while (np.linalg.norm(delta_W)>tol and n_iter<5000):\n",
    "### SGD Algorithm here\n",
    "    # Randomize samples\n",
    "    # loop over samples/batches\n",
    "        # compute the error and append it to the err list\n",
    "        \n",
    "        # compute the gradient\n",
    "\n",
    "        # update weights\n",
    "\n",
    "    ax.plot(x_draw, np.array([poly_eval(x_draw[i], w_SGD) for i in range(len(x_draw))]))\n",
    "    n_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x,t,'o')\n",
    "ax.plot(x_draw, np.array([poly_eval(x_draw[i], w_SGD) for i in range(len(x_draw))]))\n",
    "ax.plot(x_draw,y_L2,'r')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(err[::10])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
