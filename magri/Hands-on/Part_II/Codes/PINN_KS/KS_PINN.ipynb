{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-Informed Neural Network class for the KS equation\n",
    "\n",
    "NB: Note that the class is here implemented over multiple cells by defining it recursively - this is _not_ a good coding practice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Definition of the graph, loss, variables and optimizer of the PINN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KS_PINN:\n",
    "    # Define a Physics-Informed Neural Network which solves the Burgers' equation\n",
    "    def __init__(self, layers, lb, ub):\n",
    "        # layers: list of the layers for the NN\n",
    "        # lb: lower bounds of the domain\n",
    "        # ub: upper bounds of the domain\n",
    "        \n",
    "        # defines the domain of prediction for the NN\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        # Metaparameters of the NN\n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        # tf placeholders for the training data x,t are inputs, u are datapoints\n",
    "        # the None dimension allows to have an unspecified dimension\n",
    "        # (allowing to reuse the placeholder for different array size)\n",
    "        self.x_train_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.t_train_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.u_train_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "                \n",
    "        self.u_pred = self.net_u(self.x_train_tf, self.t_train_tf) \n",
    "        \n",
    "        # Loss function just based on the training set\n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_train_tf - self.u_pred))\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        \n",
    "        ## ADD CODES TO ENFORCE THE PHYSICAL LOSS AT\n",
    "        # 1. collocation points\n",
    "        self.x_coloc_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.t_coloc_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        \n",
    "        # 2. initial conditions\n",
    "        # 3. periodic boundary conditions\n",
    "        \n",
    "        ## Define the total loss function as the sum of all the loss\n",
    "        \n",
    "        ## Define the new physical optimizer\n",
    "\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Initialization and reset methods of PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KS_PINN(KS_PINN):\n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2./(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        reset = tf.global_variables_initializer()\n",
    "        self.sess.run(reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Estimation and prediction methods of the PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KS_PINN(KS_PINN):\n",
    "    # forward pass of the NN\n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "    \n",
    "    # prediction of the NN at (x,t)\n",
    "    def net_u(self, x, t):\n",
    "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
    "        \n",
    "        return u\n",
    "    \n",
    "    # estimate the prediction from the NN at points (x_pred,t_pred)\n",
    "    def predict_u(self, x_pred, t_pred):\n",
    "        u_hat = self.sess.run(self.u_pred, {self.x_train_tf: x_pred, self.t_train_tf: t_pred})  \n",
    "               \n",
    "        return u_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KS_PINN(KS_PINN):\n",
    "    def callback(self, loss):\n",
    "        print('Loss:', loss)\n",
    "    \n",
    "    # train the NN using the dataset defined by inputs=(x_train,t_train), outputs=(u_train)\n",
    "    def train_data(self, x_train, t_train, u_train):\n",
    "        tf_dict = {self.x_train_tf: x_train, self.t_train_tf: t_train,\n",
    "                   self.u_train_tf: u_train}\n",
    "                                                                                                                          \n",
    "        self.optimizer.minimize(self.sess, \n",
    "                                feed_dict = tf_dict,         \n",
    "                                fetches = [self.loss], \n",
    "                                loss_callback = self.callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training methods for the PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-9b941ba3a5e3>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-9b941ba3a5e3>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class KS_PINN(KS_PINN):\n",
    "    \n",
    "    # estimation of physical residual at (x,t)\n",
    "    # makes use of the automatic differentiator of tensorflow\n",
    "    ## DEFINE THE NEW EQUATION\n",
    "#    def net_f(self, x,t):\n",
    "#        return f\n",
    "\n",
    "    # estimate the physical residual\n",
    "#    def predict_f(self,x_pred, t_pred):\n",
    "#        f_hat = self.sess.run(self.f_pred, {self.x_coloc_tf: x_pred, self.t_coloc_tf: t_pred})\n",
    "#        \n",
    "#        return f_hat\n",
    "    \n",
    "    # train the NN using the dataset defined by inputs=(x_train,t_train), outputs=(u_train)\n",
    "    # and minimizing the physical constraints at points (x_coloc,t_coloc)\n",
    "    # DEFINE THIS TRAINING FUNCTION (should call the optimizer)\n",
    "#    def train_phys(self, ...): \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
